Here's how it all works -- majority of it written by ChatGPT :)))) --- but tweaked and fact-checked by me of course!!!

# üß† Notes RAG Assistant

Your **second brain** over your personal Obsidian-style notes. Use a local LLM via \[Ollama] with \[LlamaIndex] to semantically search, connect, and *understand* your scattered Markdown notes‚Äîno more lost insights.

---

## üöÄ What It Does

* ü§ñ **Builds a semantic index** of your `*.md` notes
* üîç **Performs similarity-based search** (not just keyword)
* üí° **Synthesizes answers** with your own writing as context

All locally, using **Ollama** (for embeddings + LLM) and **LlamaIndex** (for chunking, embedding, retrieving).

---

## üìÇ Repo Structure

```text
notes-rag-assistant/
‚îú‚îÄ‚îÄ config.yaml
‚îú‚îÄ‚îÄ notes/           ‚Üê Put your Obsidian `.md` files here
‚îú‚îÄ‚îÄ storage/         ‚Üê Holds embedding indices (auto-generated; add to `.gitignore`)
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ index_notes.py    ‚Üê Build or update the vector store
    ‚îî‚îÄ‚îÄ query_notes.py    ‚Üê Ask questions using the RAG engine
```

The key idea:

* `notes/` = your personal Markdown files
* `storage/` = where processed vectors and metadata are saved
* `config.yaml` = central configuration (paths, model names, chunking, etc.)
* `src/` = indexing and querying scripts you run directly

---

## ‚öôÔ∏è `config.yaml`

```yaml
notes_folder: "notes/"
storage_folder: "storage/"
ollama_url: "http://localhost:11434"
embed_model: "nomic-embed-text"
llm_model: "phi3:mini-4k"
chunk_size: 512
chunk_overlap: 50
similarity_top_k: 4
response_mode: "compact"
```

| Config Key         | Description                                                                                                                                                                                                                                                                     |
| ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `notes_folder`     | Path with your `.md` files (recursively). If empty, nothing gets indexed.                                                                                                                                                                                                       |
| `storage_folder`   | Directory where LlamaIndex saves the vector store. Usually permanent.                                                                                                                                                                                                           |
| `ollama_url`       | Defaults to `http://localhost:11434`‚ÄîOllama‚Äôs default REST API endpoint.                                                                                                                                                                                                        |
| `embed_model`      | Embedding model name (via Ollama). We recommend `"nomic-embed-text"` for great semantic quality and long-context encoding, outperforming OpenAI's `text-embedding-ada-002` especially on longer inputs ([LlamaIndex][1], [rocm.docs.amd.com][2], [LlamaIndex][3], [Ollama][4]). |
| `llm_model`        | LLM to run locally via Ollama‚Äî`"phi3:mini-4k"` is a lightweight and efficient \~3‚ÄØB parameter model with a 4K context window, excellent for local use ([Ollama][5], [Ollama][6]).                                                                                               |
| `chunk_size`       | Max tokens per chunk: 512 is smaller than LlamaIndex‚Äôs default (1024) and yields finer-grained semantic retrieval, with better precision on note-like content ([LlamaIndex][7], [LlamaIndex][8]).                                                                               |
| `chunk_overlap`    | How many tokens overlap between adjacent chunks (50 here instead of the default 20). This ensures continuity across splits‚Äîeven if a thought spans chunk boundaries.                                                                                                            |
| `similarity_top_k` | Number of top similar chunks returned for each query. `4` is a sweet spot if chunks are smaller‚Äîgives enough context without overwhelming the model.                                                                                                                            |
| `response_mode`    | LlamaIndex generates answers by ‚Äústitching‚Äù retrieved chunks. `"compact"` is lean and to-the-point. Other modes like `"refine"` or `"tree_summarize"` give longer, thought-out responses.                                                                                       |

---

## üß† What "Indexing" Means (Simple Version)

1. **Chunking**: each Markdown file is broken into \~512-token segments (with overlap), so you can embed them effectively.
2. **Embedding**: every chunk is converted into a high-dimensional vector using `nomic‚Äëembed‚Äëtext`.
3. **Storing**: these embeddings (plus metadata & pointers to your raw Markdown) get saved under `storage/`.
4. **Querying**: when you ask a question:

   * the question is embedded the same way,
   * LlamaIndex finds the top‚ÄëK most similar chunks by cosine similarity,
   * your local LLM (`phi3:mini‚Äë4k`) uses those retrieved chunks as context to generate a meaningful, semantically-informed answer.

üëâ This leverages meaning, context, and recall‚Äîso even if you don't remember which file or filename you wrote something in, the system finds it for you.

---

## üõ†Ô∏è Quick Start

1. **Clone the repo** and `cd` into it.

2. **Install the dependencies**:

   ```bash
   python3 -m pip install llama-index llama-index-embeddings-ollama llama-index-llms-ollama ollama
   ```

3. **Set up Ollama**:

   ```bash
   ollama server start
   ollama pull nomic-embed-text
   ollama pull phi3:mini-4k
   ```

4. **Place your Obsidian `.md` files** into the `notes/` folder (you can symlink your vault).

5. **Build the index**:

   ```bash
   python src/index_notes.py --config config.yaml
   ```

6. **Ask a question**:

   ```bash
   python src/query_notes.py --config config.yaml --question "Where did I outline the research plan?"
   ```

---

## ‚ôªÔ∏è Updating Notes

Add, edit, or remove Markdown files in `notes/` and re-run:

```bash
python src/index_notes.py --config config.yaml --refresh
```

Only changed files will be re-indexed, so it‚Äôs fast too.

---

## üí° Why I Built This

* I *write a lot* of notes‚Äîideas, essays, planning documents‚Ä¶
* Keyword search is okay, but misses the **meaning connections**
* I kept forgetting what I *already wrote*: ‚ÄúWas that idea in project X or my reading notes?‚Äù
* ‚Üí So I built a **local, semantic search + answer engine** over my actual words

This gives you full control, offline functionality, and a way to intelligently *ask your past self* what you thought about something.

---

## ‚úÖ TL;DR

* Put your `.md` files into `notes/`
* Configure paths, embedding and LLM models in `config.yaml`
* Run `index_notes.py` to build semantic index under `storage/`
* Ask natural-language questions via `cli.py` and get answers grounded in your own writing

Feel free to tweak chunk parameters, embedding/LMM models, or respond with interactive UI. Fork, pull requests, feedback‚ÄîI‚Äôd love to see what you build with this.

