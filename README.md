Here's how it all works -- majority of it written by ChatGPT :))))

```markdown
# üß† Notes RAG Assistant

This repository contains a simple **Retrieval‚ÄëAugmented Generation (RAG)** pipeline built with [LlamaIndex](/) and a locally‚Äëhosted LLM (via [Ollama](/)), designed to help you **ask smart questions about your own notes**‚Äîespecially if you use Obsidian Markdown files and often forget what you‚Äôve written.  

By indexing your note files into a vector store, the tokenizer can automatically:
1. split text into manageable ‚Äúchunks‚Äù,  
2. embed each chunk into a vector,  
3. and let you search (by *semantic similarity*, not just keywords) and synthesize answers using your local LLM.

---

## üöÄ Project Structure

```

project-root/
‚îú‚îÄ‚îÄ config.yaml
‚îú‚îÄ‚îÄ notes/            ‚Üê your Obsidian `.md` pages go here
‚îú‚îÄ‚îÄ storage/          ‚Üê vector‚Äëindices created by LlamaIndex end up here
‚îî‚îÄ‚îÄ src/
‚îú‚îÄ‚îÄ index\_notes.py   ‚Üê script to build/update index
‚îî‚îÄ‚îÄ query\_notes.py   ‚Üê script to ask questions via RAG

````

- `notes/` should contain `.md` files exported (or symlinked) from Obsidian.
- `storage/` is automatically created alongside `config.yaml` and holds index files generated by LlamaIndex (you **should not** commit actual index files).
- `src/` contains the core code: `index_notes.py` reads notes and builds/upserts an index; `query_notes.py` reads user questions and returns responses via the LLM using retrieved context.

---

## üìÅ `config.yaml`

```yaml
# Path settings
notes_folder: "notes/"
storage_folder: "storage/"

# Ollama settings
ollama_url: "http://localhost:11434"
embed_model: "nomic-embed-text"
llm_model: "phi3:mini-4k"

# Index parameters
chunk_size: 512
chunk_overlap: 50
similarity_top_k: 4
response_mode: "compact"
````

| Key                | Description                                                                                                                                                                                                                                                                                                                                               |
| ------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `notes_folder`     | Path to your Obsidian/Markdown notes. All `.md` files here (recursively) are indexed.                                                                                                                                                                                                                                                                     |
| `storage_folder`   | Where LlamaIndex saves the vector store files between runs.                                                                                                                                                                                                                                                                                               |
| `ollama_url`       | Your local Ollama server base URL (default port is 11434).                                                                                                                                                                                                                                                                                                |
| `embed_model`      | Name of the Ollama embedding model (e.g. `"nomic-embed-text"`). Must match what you pulled locally.                                                                                                                                                                                                                                                       |
| `llm_model`        | Name of the Ollama LLM model (e.g. `"phi3:mini‚Äë4k"`).                                                                                                                                                                                                                                                                                                     |
| `chunk_size`       | How many **tokens** each chunk holds (token‚Äëbased, not characters) for indexing: `"512"` means \~512 tokens per chunk. \~Default is around 1024 tokens, but smaller chunks give faster, more granular retrieval. Defaults in LlamaIndex are 1024 with overlap 20 tokens‚Äîhere we use 512/50 for better relevance and compactness ([rocm.docs.amd.com][1]). |
| `chunk_overlap`    | Number of overlapping tokens between adjacent chunks (here 50). It helps maintain context across chunk boundaries‚Äîdefaults are \~20 tokens ([LlamaIndex][2]).                                                                                                                                                                                             |
| `similarity_top_k` | After embedding a query, return this many most similar chunks (e.g. top‚Äë4). If you decrease chunk size, you‚Äôll want a higher `top_k` to compensate ([LlamaIndex][2]).                                                                                                                                                                                     |
| `response_mode`    | Controls LlamaIndex‚Äôs prompt strategy when synthesizing an answer from retrieved chunks‚Äî`"compact"` gives a concise answer, while `"refine"` or `"tree_summarize"` produce longer responses.                                                                                                                                                              |

---

## üß† What Is ‚ÄúIndexing‚Äù (in simple terms)

1. **Chunking**: your `*.md` files are split into chunks of contiguous text (\~512 tokens) with overlap to ensure continuity.
2. **Embedding**: each chunk is converted into a numeric vector using the `nomic-embed-text` model you serve through Ollama.
3. **Storage**: vectors (plus filenames and metadata) are stored in `storage/` so you don‚Äôt re‚Äëprocess everything each time.
4. **Query**: when you ask a question:

   * the question is embedded using the same model,
   * LlamaIndex finds the top‚ÄëK most similar document chunks (using cosine similarity),
   * those chunks are fed into your local LLM (`phi3:mini‚Äë4k`) as context,
   * the LLM generates a concise answer.

This avoids re‚Äëindexing every note, and lets the system surface relevant notes even if you can‚Äôt remember *which* file it‚Äôs in.

---

## ‚öôÔ∏è Installing & Running

1. Clone this repo.

2. Install required packages (e.g. Python 3.8+):

   ```bash
   pip install llama_index llama_index‚Äëembeddings‚Äëollama llama_index‚Äëllms‚Äëollama ollama-client
   ```

3. Make sure Ollama is installed & running, and you have pulled the specified models:

   ```bash
   ollama server start
   ollama pull nomic‚Äëembed‚Äëtext
   ollama pull phi3:mini‚Äë4k
   ```

4. Populate `notes/` with your Obsidian `.md` files.

5. Run indexing (initiates parse ‚Üí embed ‚Üí store vectors):

   ```bash
   python src/index_notes.py --config config.yaml
   ```

6. Ask a question:

   ```bash
   python src/query_notes.py --config config.yaml --question "What was my plan for the Rust project?"
   ```

---

## üîÑ Updating Your Notes

* Whenever you add/update/delete files in `notes/`, rerun:

  ```bash
  python src/index_notes.py --config config.yaml --refresh
  ```

  This will re‚Äëembed new or changed files (non‚Äëchanged files are kept unless you delete them).

---

## üìå Why These Defaults?

* **512 tokens + 50 overlap**: gives you enough reading‚Äêcontext (\~250‚Äì300 words) with cushion across chunk edges. In benchmarks, 512 often gave better *faithfulness*/relevance than bigger chunks, without slowing things down ‚ÑπÔ∏è ([rocm.docs.amd.com][1]).
* **Top‚Äë4 retrieval**: with smaller chunks, top‚Äë4 ensures you capture enough context across documents.
* **`"compact"` response mode**: lean responses are often all you need for note queries. If you like more detailed, multi‚Äëstage composition, `"refine"` or `"tree_summarize"` modes are also supported.

---

## üí° Why I Built This

I take a ton of notes‚Äîin Obsidian, across courses, research, and side projects‚Äîand **forget what I wrote**. Keyword search helps, but it misses the **meaning**.

By indexing notes semantically, this pipeline lets me:

* **Surface relevant ideas across files**,
* Ask follow‚Äëups like ‚Äúhow does this relate to idea‚ÄØX?‚Äù,
* Leverage an LLM to synthesize responses based on *meaning*, not keywords.

It‚Äôs essentially an ‚ÄúAI second brain‚Äù built over my own note corpus.

---

## ‚úÖ Summary

* Use your existing Obsidian `.md` files.
* Configure paths and model names in **`config.yaml`**.
* Run `index_notes.py` to build/update your index in `storage/`.
* Ask questions via `query_notes.py` to get meaningful answers from your own notes.

Feel free to fork or extend this for other embedding models, alternate LLMs in Ollama, or richer UI/tools. Pull requests and ideas are welcome‚Äîjust make sure to regenerate the index after updating the `config.yaml`.

Happy indexing, and may you forget less!

[1]: https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/inference/rag_ollama_llamaindex.html?utm_source=chatgpt.com "Constructing a RAG system using LlamaIndex and Ollama ‚Äî Tutorials for AI developers 5.0"
[2]: https://docs.llamaindex.ai/en/stable/optimizing/basic_strategies/basic_strategies/ "Basic Strategies - LlamaIndex"
