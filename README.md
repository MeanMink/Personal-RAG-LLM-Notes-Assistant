Here's how it all works -- majority of it written by ChatGPT :)))) --- but tweaked and fact-checked by me of course!!!

# ğŸ§  Notes RAG Assistant

Your **second brain** over your personal Obsidian-style notes. Use a local LLM via \[Ollama] with \[LlamaIndex] to semantically search, connect, and *understand* your scattered Markdown notesâ€”no more lost insights.

---

## ğŸš€ What It Does

* ğŸ¤– **Builds a semantic index** of your `*.md` notes
* ğŸ” **Performs similarity-based search** (not just keyword)
* ğŸ’¡ **Synthesizes answers** with your own writing as context

All locally, using **Ollama** (for embeddings + LLM) and **LlamaIndex** (for chunking, embedding, retrieving).

---

## ğŸ“‚ Repo Structure

You'd have to create your own notes/, storage/ folder and config.yaml file

```text
notes-rag-assistant/
â”œâ”€â”€ config.yaml
â”œâ”€â”€ notes/           â† Put your (Preferably Obsidian) `.md` files here (But anything works as long as its .md)
â”œâ”€â”€ storage/         â† Holds embedding indices 
â””â”€â”€ src/
    â”œâ”€â”€ indexer.py    â† Build or update the vector store
    â””â”€â”€ cli.py    â† Ask questions using the RAG engine
```

The key idea:

* `notes/` = your personal Markdown files
* `storage/` = where processed vectors and metadata are saved
* `config.yaml` = central configuration (paths, model names, chunking, etc.)
* `src/` = indexing and querying scripts you run directly

---

## âš™ï¸ `config.yaml`

```yaml
notes_folder: "notes/"
storage_folder: "storage/"
ollama_url: "http://localhost:11434"
embed_model: "nomic-embed-text"
llm_model: "phi3:mini-4k"
chunk_size: 512
chunk_overlap: 50
similarity_top_k: 4
response_mode: "compact"
```

| Config Key         | Description                                                                                                                                                                                                                                                                     |
| ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `notes_folder`     | Path with your `.md` files (recursively). If empty, nothing gets indexed.                                                                                                                                                                                                       |
| `storage_folder`   | Directory where LlamaIndex saves the vector store. Usually permanent.                                                                                                                                                                                                           |
| `ollama_url`       | Defaults to `http://localhost:11434`â€”Ollamaâ€™s default REST API endpoint.                                                                                                                                                                                                        |
| `embed_model`      | Embedding model name (via Ollama). We recommend `"nomic-embed-text"` for great semantic quality and long-context encoding, outperforming OpenAI's `text-embedding-ada-002` especially on longer inputs. |
| `llm_model`        | LLM to run locally via Ollamaâ€”`"phi3:mini-4k"` is a lightweight and efficient \~3â€¯B parameter model with a 4K context window, excellent for local use.                                                                                               |
| `chunk_size`       | Max tokens per chunk: 512 is smaller than LlamaIndexâ€™s default (1024) and yields finer-grained semantic retrieval, with better precision on note-like content.                                                                               |
| `chunk_overlap`    | How many tokens overlap between adjacent chunks (50 here instead of the default 20). This ensures continuity across splitsâ€”even if a thought spans chunk boundaries.                                                                                                            |
| `similarity_top_k` | Number of top similar chunks returned for each query. `4` is a sweet spot if chunks are smallerâ€”gives enough context without overwhelming the model, tweak as you like, but always rebuild the index.                                                                                                                            |
| `response_mode`    | LlamaIndex generates answers by â€œstitchingâ€ retrieved chunks. `"compact"` is lean and to-the-point. Other modes like `"refine"` or `"tree_summarize"` give longer, thought-out responses, tweak as you like.                                                                                       |

---

## ğŸ§  What "Indexing" Means (Simple Version)

1. **Chunking**: each Markdown file is broken into \~512-token segments (with overlap), so you can embed them effectively.
2. **Embedding**: every chunk is converted into a high-dimensional vector using `nomicâ€‘embedâ€‘text`.
3. **Storing**: these embeddings (plus metadata & pointers to your raw Markdown) get saved under `storage/`.
4. **Querying**: when you ask a question:

   * the question is embedded the same way,
   * LlamaIndex finds the topâ€‘K most similar chunks by cosine similarity,
   * your local LLM (`phi3:miniâ€‘4k`) uses those retrieved chunks as context to generate a meaningful, semantically-informed answer.

ğŸ‘‰ This leverages meaning, context, and recallâ€”so even if you don't remember which file or filename you wrote something in, the system finds it for you.

---

## ğŸ› ï¸ Quick Start (Create a virtual env if you'd like)

1. **Clone the repo** and `cd` into it.

2. **Install the dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

3. **Set up Ollama** (Download Ollama first):

   ```bash
   ollama server start
   ollama pull nomic-embed-text
   ollama pull phi3:mini-4k
   ```

4. **Place your Obsidian `.md` files** into the `notes/` folder (you can symlink your vault).

5. **Build the index**:

   ```bash
   python -m src.indexer
   ```

6. **Ask a question**:

   ```bash
   python -m src.cli -q QUESTION
   ```

---

## â™»ï¸ Updating Notes

Add, edit, or remove Markdown files in `notes/` and re-run:

```bash
python -m src.cli -r
```

Only changed files will be re-indexed, so itâ€™s fast too.

---

## ğŸ’¡ Why I Built This

* I *write a lot* of notesâ€”ideas, essays, planning documentsâ€¦
* Keyword search is okay, but misses the **meaning connections**
* I kept forgetting what I *already wrote*: â€œWas that idea in project X or my reading notes?â€
* â†’ So I built a **local, semantic search + answer engine** over my actual words

This gives you full control, offline functionality, and a way to intelligently *ask your past self* what you thought about something.

---

## âœ… TL;DR

* Put your `.md` files into `notes/`
* Configure paths, embedding and LLM models in `config.yaml`
* Run `index_notes.py` to build semantic index under `storage/`
* Ask natural-language questions via `cli.py` and get answers grounded in your own writing

Feel free to tweak chunk parameters, embedding/LMM models, or respond with interactive UI. Fork, pull requests, feedbackâ€”Iâ€™d love to see what you build with this.


